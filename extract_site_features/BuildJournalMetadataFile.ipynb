{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Journal Metadata File\n",
    "===\n",
    "\n",
    "This script is for building a journal metadata file from which info can be easily extracted.\n",
    "\n",
    "A few plausible approaches:\n",
    " - Sqlite database\n",
    " - HDF5\n",
    " - CSV\n",
    " - Dataframe, feathered\n",
    " \n",
    "Currently, we build an sqlite database file in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from html.parser import HTMLParser\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from nltk import word_tokenize\n",
    "from IPython.core.display import display, HTML\n",
    "import datetime as dt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "working_dir = \"/home/srivbane/shared/caringbridge/data/projects/qual-health-journeys/extract_site_features\"\n",
    "os.makedirs(working_dir, exist_ok=True)\n",
    "\n",
    "flattened_journal_json_filename = os.path.join(working_dir, \"journal_flat.json\")\n",
    "assert os.path.exists(flattened_journal_json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15327592 /home/srivbane/shared/caringbridge/data/projects/qual-health-journeys/extract_site_features/journal_flat.json\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l {flattened_journal_json_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def get_db():\n",
    "    db_filename = os.path.join(working_dir, \"journal_metadata.db\")\n",
    "    db = sqlite3.connect(\n",
    "            db_filename,\n",
    "            detect_types=sqlite3.PARSE_DECLTYPES\n",
    "        )\n",
    "    db.row_factory = sqlite3.Row\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = get_db()\n",
    "    db.execute(\"DROP TABLE IF EXISTS journalMetadata\")\n",
    "    create_table_command = \"\"\"\n",
    "    CREATE TABLE journalMetadata (\n",
    "          id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "          site_id INTEGER NOT NULL,\n",
    "          journal_oid TEXT NOT NULL,\n",
    "          user_id INTEGER,\n",
    "          created_at INTEGER NOT NULL,\n",
    "          updated_at INTEGER,\n",
    "          site_index INTEGER NOT NULL\n",
    "        )\n",
    "    \"\"\"\n",
    "    db.execute(create_table_command)\n",
    "finally:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN\n",
    "# It turned out to make much more sense to read from the bucket files, not the flattened json file\n",
    "return\n",
    "try:\n",
    "    db = get_db()\n",
    "    with open(flattened_journal_json_filename, 'r', encoding=\"utf-8\") as infile:\n",
    "        processed_count = 0\n",
    "        for line in tqdm(infile, total=15327592):\n",
    "            journal = json.loads(line)\n",
    "            break\n",
    "            db.execute(\n",
    "                'INSERT INTO journalMetadata (site_id, journal_oid, created_at, updated_at) VALUES (?, ?, ?, ?, ?)',\n",
    "                (site_id, journal_oid, username, annotation_type, value)\n",
    "            )\n",
    "            \n",
    "            processed_count += 1\n",
    "            if processed_count % 1000000 == 0:\n",
    "                db.commit()\n",
    "            if processed_count > 1000000:\n",
    "                break\n",
    "finally:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bucket_filename(siteId):\n",
    "    # The bucket size used in bucket_journals_by_siteId, needed to recover the appropriate bucket filename\n",
    "    bucket_size = 1000\n",
    "    working_dir = \"/home/srivbane/shared/caringbridge/data/projects/classify_health_condition/vw_experiments\"\n",
    "    sorted_journal_bucket_dir = os.path.join(working_dir, \"sorted_journal_buckets\")\n",
    "    bucket_name = \"Unknown\"\n",
    "    if siteId:\n",
    "        bucket_name = siteId // bucket_size\n",
    "    path = os.path.join(sorted_journal_bucket_dir, \"siteId_{bucket_name}.json\".format(bucket_name=bucket_name))\n",
    "    return path if os.path.exists(path) else None\n",
    "\n",
    "\n",
    "def get_journals(siteId):\n",
    "    journal_filename = get_bucket_filename(siteId)\n",
    "    journals = []\n",
    "    awaiting_first_journal = True\n",
    "    if journal_filename:\n",
    "        with open(journal_filename, 'r', encoding=\"utf8\") as infile:\n",
    "            for line in infile:\n",
    "                journal = json.loads(line.strip())\n",
    "                journal_siteId = int(journal[\"siteId\"]) if \"siteId\" in journal else None\n",
    "                if journal_siteId == siteId:\n",
    "                    journals.append(journal)\n",
    "                    if awaiting_first_journal:\n",
    "                        awaiting_first_journal = False\n",
    "                elif not awaiting_first_journal:\n",
    "                    # We have already looked at all of journals for this site\n",
    "                    break\n",
    "    return journals\n",
    "\n",
    "len(os.listdir(\"/home/srivbane/shared/caringbridge/data/projects/classify_health_condition/vw_experiments/sorted_journal_buckets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucket Files Processed: 100%|██████████| 1068/1068 [21:42<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db = get_db()\n",
    "    bucket_files_dir = \"/home/srivbane/shared/caringbridge/data/projects/classify_health_condition/vw_experiments/sorted_journal_buckets\"\n",
    "    bucket_files = os.listdir(bucket_files_dir)\n",
    "    for bucket_file in tqdm(bucket_files, desc=\"Bucket Files Processed\"):\n",
    "        with open(os.path.join(bucket_files_dir, bucket_file), 'r', encoding=\"utf-8\") as infile:\n",
    "            # load into a different list for each siteId in this file\n",
    "            site_journal_lists = collections.defaultdict(list)\n",
    "            for line in infile:\n",
    "                journal = json.loads(line)\n",
    "                site_id = int(journal[\"siteId\"])\n",
    "                site_journal_lists[site_id].append(journal)\n",
    "            for site_id in site_journal_lists.keys():\n",
    "                journal_list = site_journal_lists[site_id]\n",
    "                for i, journal in enumerate(journal_list):\n",
    "                    journal_index = i\n",
    "                    site_id = int(journal[\"siteId\"])\n",
    "                    journal_oid = journal[\"_id\"][\"$oid\"]\n",
    "                    created_at = journal[\"createdAt\"][\"$date\"]\n",
    "                    updated_at = journal[\"updatedAt\"][\"$date\"]\n",
    "                    if \"userId\" in journal:\n",
    "                        user_id = int(journal[\"userId\"])\n",
    "                        db.execute(\n",
    "                        'INSERT INTO journalMetadata (site_id, journal_oid, created_at, updated_at, site_index, user_id) VALUES (?, ?, ?, ?, ?, ?)',\n",
    "                        (site_id, journal_oid, created_at, updated_at, journal_index, user_id))\n",
    "                    else:\n",
    "                        db.execute(\n",
    "                        'INSERT INTO journalMetadata (site_id, journal_oid, created_at, updated_at, site_index) VALUES (?, ?, ?, ?, ?)',\n",
    "                        (site_id, journal_oid, created_at, updated_at, journal_index))\n",
    "        db.commit()  # commit after each file is processed\n",
    "finally:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCREATE INDEX journalMetadata_siteId ON journalMetadata (site_id);\\nCREATE INDEX journalMetadata_journalOid ON journalMetadata (journal_oid);\\nCREATE INDEX journalMetadata_siteId_journalOid ON journalMetadata (site_id, journal_oid);\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I also manually created the following indexes:\n",
    "\"\"\"\n",
    "CREATE INDEX journalMetadata_siteId ON journalMetadata (site_id);\n",
    "CREATE INDEX journalMetadata_journalOid ON journalMetadata (journal_oid);\n",
    "CREATE INDEX journalMetadata_siteId_journalOid ON journalMetadata (site_id, journal_oid);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Full Journal Text File\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build a separate database containing the journal text\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "def get_db():\n",
    "    db_filename = os.path.join(working_dir, \"journal_text.db\")\n",
    "    db = sqlite3.connect(\n",
    "            db_filename,\n",
    "            detect_types=sqlite3.PARSE_DECLTYPES\n",
    "        )\n",
    "    db.row_factory = sqlite3.Row\n",
    "    return db\n",
    "\n",
    "try:\n",
    "    db = get_db()\n",
    "    db.execute(\"DROP TABLE IF EXISTS journalText\")\n",
    "    create_table_command = \"\"\"\n",
    "    CREATE TABLE journalText (\n",
    "          id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "          site_id INTEGER NOT NULL,\n",
    "          journal_oid TEXT NOT NULL,\n",
    "          body TEXT\n",
    "        )\n",
    "    \"\"\"\n",
    "    db.execute(create_table_command)\n",
    "finally:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bucket Files Processed: 100%|██████████| 1068/1068 [22:43<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# load the journal json, writing out the body text of the journals to the database\n",
    "try:\n",
    "    db = get_db()\n",
    "    bucket_files_dir = \"/home/srivbane/shared/caringbridge/data/projects/classify_health_condition/vw_experiments/sorted_journal_buckets\"\n",
    "    bucket_files = os.listdir(bucket_files_dir)\n",
    "    for bucket_file in tqdm(bucket_files, desc=\"Bucket Files Processed\"):\n",
    "        with open(os.path.join(bucket_files_dir, bucket_file), 'r', encoding=\"utf-8\") as infile:\n",
    "            # load into a different list for each siteId in this file\n",
    "            site_journal_lists = collections.defaultdict(list)\n",
    "            for line in infile:\n",
    "                journal = json.loads(line)\n",
    "                site_id = int(journal[\"siteId\"])\n",
    "                site_journal_lists[site_id].append(journal)\n",
    "            for site_id in site_journal_lists.keys():\n",
    "                journal_list = site_journal_lists[site_id]\n",
    "                for i, journal in enumerate(journal_list):\n",
    "                    journal_index = i\n",
    "                    site_id = int(journal[\"siteId\"])\n",
    "                    journal_oid = journal[\"_id\"][\"$oid\"]\n",
    "                    body = journal[\"body\"] if \"body\" in journal else None\n",
    "                    db.execute(\n",
    "                    'INSERT INTO journalText (site_id, journal_oid, body) VALUES (?, ?, ?)',\n",
    "                    (site_id, journal_oid, body))\n",
    "        db.commit()  # commit after each file is processed\n",
    "finally:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    db = get_db()\n",
    "    index_command = \"\"\"\n",
    "    CREATE INDEX journalText_siteId_journalOid ON journalText (site_id, journal_oid);\n",
    "    \"\"\"\n",
    "    db.execute(index_command)\n",
    "finally:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
