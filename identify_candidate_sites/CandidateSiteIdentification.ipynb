{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate Site Identification\n",
    "===\n",
    "\n",
    "Using the restrictions we've identified, how many sites are actually available for analysis?\n",
    "\n",
    "Using these sampling restrictions:\n",
    " - Health condition is \"cancer\"\n",
    " - Site's createdAt > 2009-01-01\n",
    " - Site's updatedAt - createdAt < 1 week i.e. not edited much after the site creation time\n",
    " - Last journal createdAt - first journal createdAt > 1 year\n",
    " - Num journals in the site >= 10\n",
    " - Created by the patient\n",
    "     * Potentially, using site's \"isForSelf\" key.\n",
    "     * Potentially, looking for personal pronouns in the site description and most of the journals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from html.parser import HTMLParser\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "from nltk import word_tokenize\n",
    "from IPython.core.display import display, HTML\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The bucket size used in bucket_journals_by_siteId, needed to recover the appropriate bucket filename\n",
    "BUCKET_SIZE = 1000\n",
    "\n",
    "def get_bucket_filename(siteId):\n",
    "    working_dir = \"/home/srivbane/shared/caringbridge/data/projects/classify_health_condition/vw_experiments\"\n",
    "    sorted_journal_bucket_dir = os.path.join(working_dir, \"sorted_journal_buckets\")\n",
    "    bucket_name = \"Unknown\"\n",
    "    if siteId:\n",
    "        bucket_name = siteId // BUCKET_SIZE\n",
    "    path = os.path.join(sorted_journal_bucket_dir, \"siteId_{bucket_name}.json\".format(bucket_name=bucket_name))\n",
    "    return path if os.path.exists(path) else None\n",
    "    \n",
    "def get_journals(siteId):\n",
    "    journal_filename = get_bucket_filename(siteId)\n",
    "    journals = []\n",
    "    awaiting_first_journal = True\n",
    "    if journal_filename:\n",
    "        with open(journal_filename, 'r', encoding=\"utf8\") as infile:\n",
    "            for line in infile:\n",
    "                journal = json.loads(line.strip())\n",
    "                journal_siteId = int(journal[\"siteId\"]) if \"siteId\" in journal else None\n",
    "                if journal_siteId == siteId:\n",
    "                    journals.append(journal)\n",
    "                    if awaiting_first_journal:\n",
    "                        awaiting_first_journal = False\n",
    "                elif not awaiting_first_journal:\n",
    "                    # We have already looked at all of journals for this site\n",
    "                    break\n",
    "    return journals\n",
    "\n",
    "def get_journal_times(siteId):\n",
    "    journal_filename = get_bucket_filename(siteId)\n",
    "    first_journal_timestamp = -1\n",
    "    last_journal_timestamp = -1\n",
    "    num_journals_found = 0\n",
    "    awaiting_first_journal = True\n",
    "    if journal_filename:\n",
    "        with open(journal_filename, 'r', encoding=\"utf8\") as infile:\n",
    "            for line in infile:\n",
    "                journal = json.loads(line.strip())\n",
    "                journal_siteId = int(journal[\"siteId\"]) if \"siteId\" in journal else None\n",
    "                if journal_siteId == siteId:\n",
    "                    num_journals_found += 1\n",
    "                    if \"createdAt\" not in journal:\n",
    "                        continue\n",
    "                    if awaiting_first_journal:\n",
    "                        first_journal_timestamp = get_timestamp(journal[\"createdAt\"])\n",
    "                        awaiting_first_journal = False\n",
    "                    last_journal_timestamp = get_timestamp(journal[\"createdAt\"])\n",
    "                elif not awaiting_first_journal:\n",
    "                    # We have already looked at all of journals for this site\n",
    "                    break\n",
    "    return first_journal_timestamp, last_journal_timestamp, num_journals_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_timestamp(date_dict, min_year=2004, invalid_date_return = None):\n",
    "    \"\"\"\n",
    "    :return a string representing the date, or invalid_date_repr if the date is invalid\n",
    "    \"\"\"\n",
    "    \n",
    "    # expect the date_dict to contain the date as a number in the \"$date\" field\n",
    "    if \"$date\" not in date_dict:\n",
    "        return invalid_date_return\n",
    "    \n",
    "    # convert the date to unix time (i.e. seconds since the unix epoch)\n",
    "    created_at_utc = date_dict[\"$date\"] / 1000\n",
    "    \n",
    "    # check that the date occurs past some minimum year\n",
    "    earliest_valid_date = dt.datetime(year=min_year,month=1,day=1)\n",
    "    earliest_valid_date_timestamp = earliest_valid_date.replace(tzinfo=dt.timezone.utc).timestamp()\n",
    "    if created_at_utc < earliest_valid_date_timestamp:\n",
    "        return invalid_date_return\n",
    "    \n",
    "    return created_at_utc\n",
    "    #datetime.utcfromtimestamp(created_at_utc).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def get_site_data(site_json):\n",
    "    site = site_json\n",
    "    if \"_id\" in site and site[\"_id\"] is not None:\n",
    "        siteId = int(site[\"_id\"])\n",
    "\n",
    "        name = site[\"name\"] if \"name\" in site else \"\"\n",
    "        firstName = site[\"firstName\"] if \"firstName\" in site else \"\"\n",
    "        lastName = site[\"lastName\"] if \"lastName\" in site else \"\"\n",
    "        title = site[\"title\"] if \"title\" in site else \"\"\n",
    "        description = site[\"description\"] if \"description\" in site else \"\"\n",
    "        healthCondition = site[\"healthCondition\"] if \"healthCondition\" in site else {}\n",
    "        createdAt = site[\"createdAt\"] if \"createdAt\" in site else {}\n",
    "        updatedAt = site[\"updatedAt\"] if \"updatedAt\" in site else {}\n",
    "        visits =  int(site[\"visits\"]) if \"visits\" in site else None\n",
    "        numJournals = int(site[\"numJournals\"]) if \"numJournals\" in site else None\n",
    "        numAmps = int(site[\"numAmps\"]) if \"numAmps\" in site else None\n",
    "        numTributes = int(site[\"numTributes\"]) if \"numTributes\" in site else None\n",
    "        numGuestbooks = int(site[\"numGuestbooks\"]) if \"numGuestbooks\" in site else None\n",
    "        numTasks = int(site[\"numTasks\"]) if \"numTasks\" in site else None\n",
    "        numPhotos = int(site[\"numPhotos\"]) if \"numPhotos\" in site else None\n",
    "        privacy = site[\"privacy\"] if \"privacy\" in site else \"unknown\"\n",
    "        \n",
    "        if \"category\" in healthCondition and healthCondition[\"category\"].lower() == \"cancer\":\n",
    "            created_at_utc = get_timestamp(createdAt, min_year = 2009)\n",
    "            updated_at_utc = get_timestamp(updatedAt)\n",
    "            one_week_secs = 60 * 60 * 24 * 7\n",
    "            if created_at_utc and (not updated_at_utc or \\\n",
    "                                   (created_at_utc > updated_at_utc \\\n",
    "                                    or updated_at_utc - created_at_utc < one_week_secs)):\n",
    "                isForSelf = int(site[\"isForSelf\"]) if \"isForSelf\" in site else 0\n",
    "                if isForSelf == 1:                \n",
    "                    # Get the journals\n",
    "                    first_journal_timestamp, last_journal_timestamp, num_journals_found = get_journal_times(siteId)\n",
    "                    if num_journals_found >= 10:\n",
    "                        one_year_secs = 60 * 60 * 24 * 365\n",
    "                        if first_journal_timestamp and last_journal_timestamp and last_journal_timestamp - first_journal_timestamp > one_year_secs:\n",
    "                            journals = get_journals(siteId)\n",
    "                            site_json[\"journals\"] = journals\n",
    "                            return siteId, site_json\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 / 588210 (Valid sites: 0)\n",
      "20000 / 588210 (Valid sites: 0)\n",
      "30000 / 588210 (Valid sites: 0)\n",
      "40000 / 588210 (Valid sites: 0)\n",
      "50000 / 588210 (Valid sites: 0)\n",
      "60000 / 588210 (Valid sites: 0)\n",
      "70000 / 588210 (Valid sites: 0)\n",
      "80000 / 588210 (Valid sites: 0)\n",
      "90000 / 588210 (Valid sites: 0)\n",
      "100000 / 588210 (Valid sites: 0)\n",
      "110000 / 588210 (Valid sites: 0)\n",
      "120000 / 588210 (Valid sites: 0)\n",
      "130000 / 588210 (Valid sites: 0)\n",
      "140000 / 588210 (Valid sites: 0)\n",
      "150000 / 588210 (Valid sites: 0)\n",
      "160000 / 588210 (Valid sites: 0)\n",
      "170000 / 588210 (Valid sites: 0)\n",
      "180000 / 588210 (Valid sites: 14)\n",
      "190000 / 588210 (Valid sites: 39)\n",
      "200000 / 588210 (Valid sites: 81)\n",
      "210000 / 588210 (Valid sites: 119)\n",
      "220000 / 588210 (Valid sites: 155)\n",
      "230000 / 588210 (Valid sites: 193)\n",
      "240000 / 588210 (Valid sites: 228)\n",
      "250000 / 588210 (Valid sites: 256)\n",
      "260000 / 588210 (Valid sites: 294)\n",
      "270000 / 588210 (Valid sites: 319)\n",
      "280000 / 588210 (Valid sites: 360)\n",
      "290000 / 588210 (Valid sites: 407)\n",
      "300000 / 588210 (Valid sites: 458)\n",
      "310000 / 588210 (Valid sites: 489)\n",
      "320000 / 588210 (Valid sites: 521)\n",
      "330000 / 588210 (Valid sites: 557)\n",
      "340000 / 588210 (Valid sites: 573)\n",
      "350000 / 588210 (Valid sites: 598)\n",
      "360000 / 588210 (Valid sites: 637)\n",
      "370000 / 588210 (Valid sites: 678)\n",
      "380000 / 588210 (Valid sites: 718)\n",
      "390000 / 588210 (Valid sites: 738)\n",
      "400000 / 588210 (Valid sites: 753)\n",
      "410000 / 588210 (Valid sites: 775)\n",
      "420000 / 588210 (Valid sites: 792)\n",
      "430000 / 588210 (Valid sites: 808)\n",
      "440000 / 588210 (Valid sites: 826)\n",
      "450000 / 588210 (Valid sites: 854)\n",
      "460000 / 588210 (Valid sites: 877)\n",
      "470000 / 588210 (Valid sites: 887)\n",
      "480000 / 588210 (Valid sites: 924)\n",
      "490000 / 588210 (Valid sites: 930)\n",
      "500000 / 588210 (Valid sites: 930)\n",
      "510000 / 588210 (Valid sites: 930)\n",
      "520000 / 588210 (Valid sites: 930)\n",
      "530000 / 588210 (Valid sites: 930)\n",
      "540000 / 588210 (Valid sites: 930)\n",
      "550000 / 588210 (Valid sites: 930)\n",
      "560000 / 588210 (Valid sites: 930)\n",
      "570000 / 588210 (Valid sites: 930)\n",
      "580000 / 588210 (Valid sites: 930)\n",
      "588210 / 588210 (Valid sites: 930)\n"
     ]
    }
   ],
   "source": [
    "raw_data_dir = \"/home/srivbane/shared/caringbridge/data/raw\"\n",
    "site_filename = os.path.join(raw_data_dir, \"site_scrubbed.json\")\n",
    "\n",
    "working_dir = \"/home/srivbane/shared/caringbridge/data/projects/qual-health-conditions/identify_candidate_sites\"\n",
    "output_filename = os.path.join(working_dir, \"site_selected.json\")\n",
    "\n",
    "i = 0\n",
    "site_count = 0\n",
    "site_list = []\n",
    "with open(site_filename, 'r', encoding=\"utf8\") as infile:\n",
    "    with open(output_filename, 'w', encoding=\"utf8\") as outfile:\n",
    "        for line in infile:\n",
    "            i += 1\n",
    "            site = json.loads(line.strip())\n",
    "            site_data = get_site_data(site)\n",
    "            if site_data:\n",
    "                site_count += 1\n",
    "                siteId, site_json = site_data\n",
    "                site_list.append(siteId)\n",
    "                json.dump(site_json, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "            if i % 10000 == 0:\n",
    "                print(\"{i} / 588210 (Valid sites: {site_count})\".format(i=i, site_count=site_count))\n",
    "print(\"{i} / 588210 (Valid sites: {site_count})\".format(i=i, site_count=site_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "site_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
