{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline keyword-based approach\n",
    "\n",
    "Two questions of general interest:\n",
    " - How many words are needed to get the best possible identification of responsibilities? (Alternately, how many words are needed to match the performance of the best-performing ML model?)\n",
    " - Given *k* words, what is the best performance that could be achieved?\n",
    " \n",
    "Focus on second question, implementing the Partial Set Cover (or Partial Cover, or k-Partial Set Cover, or Max k-Cover) algorithm.\n",
    " \n",
    "https://en.wikipedia.org/wiki/Maximum_coverage_problem\n",
    " \n",
    "https://en.wikipedia.org/wiki/Set_cover_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../annotation_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibility import *\n",
    "from phase import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.monitor_interval = 0\n",
    "from nltk import word_tokenize, bigrams, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import re\n",
    "import time\n",
    "from utils import *\n",
    "from db import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/srivbane/shared/caringbridge/data/projects/qual-health-journeys/classification/baseline_set_cover\"\n",
    "assert os.path.exists(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1895"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_subset = high_irr_responsibility_labels\n",
    "annotated_df_resp = get_annotated_responsibility_df_fixed(conflict_score_cost=0.1, resp_subset=resp_subset)\n",
    "len(annotated_df_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df_resp.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['communicating', 'info_filtering', 'clinical_decisions', 'preparation', 'symptom_management', 'coordinating_support', 'sharing_medical_info', 'compliance', 'managing_transitions', 'financial_management', 'continued_monitoring', 'giving_back', 'behavior_changes']\n"
     ]
    }
   ],
   "source": [
    "print(responsibility_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coordinating_support', 'sharing_medical_info', 'compliance', 'financial_management', 'giving_back', 'behavior_changes']\n"
     ]
    }
   ],
   "source": [
    "print(resp_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir_phase = '/home/srivbane/shared/caringbridge/data/projects/qual-health-journeys/classification/phases/vw'\n",
    "assert os.path.exists(working_dir_phase)\n",
    "phases_df_filepath = os.path.join(working_dir_phase, 'full_df.pkl')\n",
    "phases_df = pd.read_pickle(phases_df_filepath)\n",
    "annotated_df_phase = phases_df[phases_df.is_annotated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158109"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phases_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pretreatment', 'treatment', 'end_of_life', 'cured']\n"
     ]
    }
   ],
   "source": [
    "print(phase_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def commonize_token(token):    \n",
    "    token = token.strip()\n",
    "    token = re.sub('\\d', '0', token)\n",
    "    token = re.sub('[^\\w\\$\\.\\']', '|', token)\n",
    "    token = token.lower()\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_grams(text, n_values=[1], remove_stop = True):\n",
    "    tokens = word_tokenize(text)\n",
    "    if remove_stop:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [tok for tok in tokens if tok not in stop_words]\n",
    "    tokens = [commonize_token(tok) for tok in tokens]\n",
    "    grams = []\n",
    "    for n in n_values:\n",
    "        grams += [' '.join(i) for i in ngrams(tokens, n)]\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_uniquely_positive_ngrams(journals, grams_column, n_values=[1], labels = responsibility_labels):\n",
    "    lines = []\n",
    "    lines.append('Label' + ' '*10 + 'Pos Journal Count' + ' '*10 + 'Only Positive Words')\n",
    "    # lines.append(f'{'Label':23}  {'Pos Journal Count'}  {'Only Positive Words':20}')\n",
    "    lines.append(\"=\"*80)\n",
    "    upw_dict = dict()\n",
    "    for resp_label in labels:\n",
    "        pos_word_set = set()\n",
    "        neg_word_set = set()\n",
    "        \n",
    "        for i in range(len(journals)):\n",
    "        #for i in tqdm(range(len(journals)), desc=resp_label):\n",
    "            row = journals.iloc[i]\n",
    "            is_positive = row[resp_label + '_score'] > 0.5\n",
    "            journal_text = row['journal_text']\n",
    "            grams = row[grams_column]\n",
    "            \n",
    "            if is_positive:\n",
    "                pos_word_set.update(grams)\n",
    "            else:\n",
    "                neg_word_set.update(grams)\n",
    "                \n",
    "        uniquely_positive_words = pos_word_set - neg_word_set\n",
    "        uniquely_positive_words_count = len(uniquely_positive_words)\n",
    "        upw_dict[resp_label] = uniquely_positive_words\n",
    "        pos_journal_count = len(journals[journals[resp_label + '_score'] > 0.5])\n",
    "        lines.append(f\"{resp_label:23}  {pos_journal_count:17}  {uniquely_positive_words_count:20}\")\n",
    "#    for line in lines:\n",
    "#        print(line)\n",
    "    return(upw_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upw_dict_1 = get_uniquely_positive_ngrams(annotated_df, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upw_dict_2 = get_uniquely_positive_ngrams(annotated_df, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upw_dict_1_2 = get_uniquely_positive_ngrams(annotated_df, [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_pairs(a):\n",
    "    a = list(a)\n",
    "    pairs = set()\n",
    "    for i in range(len(a)):\n",
    "        for j in range(i+1, len(a)):\n",
    "            pairs.add(frozenset([a[i], a[j]]))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_uniquely_positive_stopgrams(labels):\n",
    "    lines = []\n",
    "    lines.append('Label' + ' '*10 + 'Pos Journal Count' + ' '*10 + 'Only Positive Words')\n",
    "    # lines.append(f'{'Label':23}  {'Pos Journal Count'}  {'Only Positive Words':20}')\n",
    "    lines.append(\"=\"*80)\n",
    "    for resp_label in labels:\n",
    "        uniquely_positive_words = set()\n",
    "        pos_word_set = set()\n",
    "        neg_word_set = set()\n",
    "        for i in tqdm(range(len(annotated_df)), desc=resp_label):\n",
    "            row = annotated_df.iloc[i]\n",
    "            is_positive = row[resp_label + '_score'] > 0.5\n",
    "            journal_text = row['journal_text']\n",
    "            grams_set = set(get_grams(journal_text, [1]))\n",
    "            grams_pairs = get_pairs(grams_set)\n",
    "            \n",
    "            if is_positive:\n",
    "                pos_word_set.update(grams_pairs)\n",
    "            else:\n",
    "                neg_word_set.update(grams_pairs)\n",
    "            \n",
    "        uniquely_positive_words = pos_word_set - neg_word_set\n",
    "        uniquely_positive_words_count = len(uniquely_positive_words)\n",
    "        pos_journal_count = len(annotated_df[annotated_df[resp_label + '_score'] > 0.5])\n",
    "        \n",
    "        output_filename = os.path.join(working_dir, resp_label + \"_stop_bigram_counts.csv\")\n",
    "        with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "            for pair in uniquely_positive_words:\n",
    "                outfile.write(' '.join(pair) + '\\n')\n",
    "        print(uniquely_positive_words_count)\n",
    "        lines.append(f\"{resp_label:23}  {pos_journal_count:17}  {uniquely_positive_words_count:20}\")\n",
    "    for line in lines:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#upw_dict_stopgrams = get_uniquely_positive_stopgrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Generate a dictionary of 'sets' for each responsibility\n",
    "# where each set is labeled as a uniquely positive word\n",
    "# and the elements of that set are the positive journals that contain that word\n",
    "\n",
    "# \"sharing\" = set( (site_id, journal_oid), ... 30 journals )\n",
    "\n",
    "# \"medical\" = set( (site_id, journal_oid), ... 20 journals )\n",
    "# \"information\" = set( (site_id, journal_oid), ... 5 journals )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158109/158109 [13:32<00:00, 194.69it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_global_counts():\n",
    "    valid_sites = get_valid_sites_filtered()\n",
    "    texts = []\n",
    "    for site_id in tqdm(valid_sites):\n",
    "        journals = get_journal_info(site_id)\n",
    "        for journal in journals:\n",
    "            journal_text = get_journal_text_representation(journal)\n",
    "            if journal_text is None:\n",
    "                continue\n",
    "            texts.append(journal_text)\n",
    "    return get_global_counts_from_texts(texts)\n",
    "\n",
    "def get_global_counts_from_texts(texts):\n",
    "    global_word_counts = defaultdict(int)\n",
    "    for text in tqdm(texts):\n",
    "        journal_tokens = word_tokenize(text)\n",
    "        for token in journal_tokens:\n",
    "            token = commonize_token(token)\n",
    "            global_word_counts[token] += 1 \n",
    "\n",
    "    # save counts\n",
    "    global_counts_filename = os.path.join(working_dir, 'global_counts.tsv')\n",
    "    with open(global_counts_filename, 'w', encoding='utf-8') as outfile:\n",
    "        for token in global_word_counts:\n",
    "            outfile.write(str(token) + '\\t' + str(global_word_counts[token]) + '\\n')\n",
    "            \n",
    "# no need to load from the database since we have all of the journal texts in the phases_df already\n",
    "get_global_counts_from_texts(phases_df.journal_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load counts  (created by get_global_counts_from_texts)\n",
    "global_word_counts = defaultdict(int)\n",
    "global_counts_filename = os.path.join(working_dir, 'global_counts.tsv')\n",
    "with open(global_counts_filename, 'r', encoding='utf-8') as infile:\n",
    "    for line in infile:\n",
    "        token, count = line.split(\"\\t\")\n",
    "        global_word_counts[token] = int(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('|', 3787365),\n",
       " ('.', 2832535),\n",
       " ('i', 2677716),\n",
       " ('the', 1977410),\n",
       " ('to', 1758365),\n",
       " ('and', 1614532),\n",
       " ('my', 1120371),\n",
       " ('a', 1093091),\n",
       " ('of', 877838),\n",
       " ('it', 787439),\n",
       " ('that', 767714),\n",
       " ('for', 633821),\n",
       " ('in', 624474),\n",
       " ('is', 616512),\n",
       " ('newline', 596803),\n",
       " ('have', 557908),\n",
       " ('me', 521426),\n",
       " ('was', 518121),\n",
       " ('so', 432927),\n",
       " ('with', 420906)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the 20 most frequent words\n",
    "sorted([(token, count) for token, count in global_word_counts.items()], reverse=True, key=lambda tup: tup[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def break_tie(equal_words):\n",
    "    if len(equal_words) == 1:\n",
    "        return list(equal_words.keys())[0]\n",
    "    \n",
    "    max_count = max(equal_words.values())\n",
    "    equal_words = {word:equal_words[word] for word in equal_words if equal_words[word] == max_count}\n",
    "    if len(equal_words) == 1:\n",
    "        return list(equal_words.keys())[0]\n",
    "    \n",
    "    return np.random.choice(list(equal_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def remove_non_upw(tokens, upw):\n",
    "    return {word for word in tokens if word in upw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_journal_count(word, journals):\n",
    "    count = 0\n",
    "    for journal in journals:\n",
    "        if word in journal:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def max_k_cover(journals, k, uniquely_positive_words, resp_label, n_values=[1]):\n",
    "    upw_copy = set(uniquely_positive_words)\n",
    "    \n",
    "    word_list = []\n",
    "    #TODO Generate a list of words\n",
    "    # Breaking ties: at many stages in the algorithm, you'll have multiple words that give you the same improvement in terms of number of documents covered\n",
    "    # When this happens, you should choose randomly.  BUT, we'll want to change this in the future....\n",
    "    \n",
    "    journals = journals.apply(set)\n",
    "    \n",
    "    journals_copy = journals.copy(deep=True)\n",
    "    \n",
    "    # Remove all words that are not uniquely positive words\n",
    "    journals = journals.apply(remove_non_upw, args=(upw_copy,))\n",
    "    \n",
    "    for i in range(k):\n",
    "    #for i in tqdm(range(k), desc=resp_label):\n",
    "        start = time.time()\n",
    "        token_counts = Counter([token for journal in journals for token in journal])\n",
    "        if not token_counts:\n",
    "            break\n",
    "        max_count = max(token_counts.values())\n",
    "        max_word_list = [word for word in token_counts if token_counts[word] == max_count]\n",
    "        max_word_dict = {word:global_word_counts[word] for word in max_word_list}\n",
    "        max_word = break_tie(max_word_dict)\n",
    "\n",
    "        word_list.append(max_word)\n",
    "        journals = journals[journals.apply(lambda x: max_word not in x)]\n",
    "        \n",
    "    assert len(word_list) <= k\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def set_cover(n_values=[1]):\n",
    "    upw_dict = upw_dict_1\n",
    "    if (n_values == [2]):\n",
    "        upw_dict = upw_dict_2\n",
    "    if (n_values == [1,2]):\n",
    "        upw_dict = upw_dict_1_2\n",
    "    \n",
    "    k_values = [1, 3, 5, 10, 50, 100, 500, 1000]\n",
    "    print(k_values)\n",
    "    for resp_label in responsibility_labels:\n",
    "        journals = annotated_df.loc[annotated_df[resp_label + '_score'] > 0.5, 'journal_text'] \n",
    "        output_filename = os.path.join(working_dir, resp_label + \"_nostop_wordlists.csv\")\n",
    "        with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "            k = max(k_values)\n",
    "            word_list = max_k_cover(journals, k, upw_dict[resp_label], resp_label, n_values)\n",
    "            string_to_write = str(k) + '\\n' + '\\n'.join(word_list)\n",
    "            outfile.write(string_to_write)\n",
    "    end = time.time()\n",
    "#set_cover()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def remove_non_upw_stop_bigrams(tokens, upw):\n",
    "    print(upw)\n",
    "    return {word for word in tokens if word in upw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def max_k_cover_stopgrams(journals, k, uniquely_positive_words, resp_label):\n",
    "    upw_copy = set(uniquely_positive_words)\n",
    "    \n",
    "    word_list = []\n",
    "    #TODO Generate a list of words\n",
    "    # Breaking ties: at many stages in the algorithm, you'll have multiple words that give you the same improvement in terms of number of documents covered\n",
    "    # When this happens, you should choose randomly.  BUT, we'll want to change this in the future....\n",
    "    \n",
    "    journals = journals.apply(get_grams)\n",
    "    journals = journals.apply(set)\n",
    "    journals = journals.apply(get_pairs)\n",
    "    \n",
    "    journals_copy = journals.copy(deep=True)\n",
    "    \n",
    "    # Remove all words that are not uniquely positive words\n",
    "    journals = journals.apply(remove_non_upw, args=(upw_copy,))\n",
    "    \n",
    "    #for i in range(k):\n",
    "    for i in tqdm(range(k), desc=resp_label):        \n",
    "        token_counts = Counter([token for journal in journals for token in journal])\n",
    "        if not token_counts:\n",
    "            break\n",
    "        max_count = max(token_counts.values())\n",
    "        max_word_list = [word for word in token_counts if token_counts[word] == max_count]\n",
    "        max_word_dict = {word:get_global_count(word, journals_copy) for word in max_word_list}\n",
    "        max_word = break_tie(max_word_dict)\n",
    "        \n",
    "        word_list.append(str(max_word_dict[max_word]) + '\\t' + ' '.join(max_word))\n",
    "        \n",
    "        journals = journals[journals.apply(lambda x: max_word not in x)]\n",
    "    \n",
    "    assert len(word_list) <= k\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def set_cover_stopgrams():\n",
    "    start = time.time()\n",
    "    k_values = [1, 3, 5, 10, 50, 100, 500, 1000]\n",
    "    print(k_values)\n",
    "    for resp_label in responsibility_labels:\n",
    "        uniquely_positive_stop_bigrams = set()\n",
    "        journals = annotated_df.loc[annotated_df[resp_label + '_score'] > 0.5, 'journal_text']\n",
    "\n",
    "        output_filename = os.path.join(working_dir, resp_label + \"_stop_bigram_counts.csv\")\n",
    "        with open(output_filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                uniquely_positive_stopgrams.add(frozenset(line.split()))\n",
    "        \n",
    "\n",
    "        output_filename = os.path.join(working_dir, resp_label + \"_stop_bigrams_wordlists.csv\")\n",
    "        with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "            k = max(k_values)\n",
    "            word_list = max_k_cover_stop_bigrams(journals, k, uniquely_positive_stop_bigrams, resp_label)\n",
    "            string_to_write = str(k) + '\\n' + '\\n'.join(word_list)\n",
    "            outfile.write(string_to_write)\n",
    "    end = time.time()\n",
    "    print(str(end-start))\n",
    "#set_cover_stopgrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def classify_journal(journal_grams, word_list):\n",
    "    return any(word in journal_grams for word in word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def eval_model(gram_column, train_journals, test_journals, train_true, test_true, word_list):\n",
    "    \n",
    "    model_eval = [np.nan, np.nan, np.nan, np.nan]\n",
    "    \n",
    "    train_predicted = train_journals[gram_column].apply(classify_journal, args=(word_list,))\n",
    "    test_predicted = test_journals[gram_column].apply(classify_journal, args=(word_list,))\n",
    "    \n",
    "    model_eval[0] = sklearn.metrics.recall_score(train_true, train_predicted) #Train recall\n",
    "    model_eval[2] = sklearn.metrics.recall_score(test_true, test_predicted) #Test recall\n",
    "    \n",
    "    if train_predicted.any():\n",
    "        model_eval[1] = sklearn.metrics.fbeta_score(train_true, train_predicted, 1) #Train f1\n",
    "    \n",
    "    if test_predicted.any():\n",
    "        model_eval[3] = sklearn.metrics.fbeta_score(test_true, test_predicted, 1) #Test f1\n",
    "    \n",
    "    return model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(journals, labels):\n",
    "    dfs = []\n",
    "    for label in labels:\n",
    "        resp_list = []    \n",
    "\n",
    "        kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "\n",
    "        #for train_indices, test_indices in kf.split(journals):\n",
    "        for train_indices, test_indices in tqdm(kf.split(journals), desc=label):\n",
    "            train_journals = journals.iloc[train_indices]\n",
    "            test_journals = journals.iloc[test_indices]\n",
    "            \n",
    "            train_true = train_journals[label + '_score'] > 0.5\n",
    "            test_true = test_journals[label + '_score'] > 0.5\n",
    "            \n",
    "            if not any(train_true) or not any(test_true):\n",
    "                for k in [10,100]:\n",
    "                    for remove_stop in [True, False]:\n",
    "                        resp_list.append([label, k, remove_stop, 'unigram'] + [np.nan]*4)\n",
    "                        resp_list.append([label, k, remove_stop, 'bigram'] + [np.nan]*4)\n",
    "                continue\n",
    "            \n",
    "            start = time.time()\n",
    "            for remove_stop in [True, False]:\n",
    "                uni = 'uni_nostop'\n",
    "                bi = 'bi_nostop'\n",
    "                if remove_stop:\n",
    "                    uni = 'uni_stop'\n",
    "                    bi = 'bi_stop'\n",
    "                \n",
    "                upw_dict_1 = get_uniquely_positive_ngrams(train_journals, uni, [1], [label])\n",
    "                upw_dict_2 = get_uniquely_positive_ngrams(train_journals, bi, [2], [label])\n",
    "                full_word_list_1 = max_k_cover(train_journals[uni], 100, upw_dict_1[label], label)\n",
    "                full_word_list_2 = max_k_cover(train_journals[bi], 100, upw_dict_2[label], label)\n",
    "                \n",
    "                for k in [10,100]:   \n",
    "                    unigram = eval_model(uni, train_journals, test_journals, train_true, test_true, full_word_list_1[:k])\n",
    "                    resp_list.append([label, k, remove_stop, 'unigram'] + unigram)\n",
    "                    \n",
    "                    bigram = eval_model(bi, train_journals, test_journals, train_true, test_true, full_word_list_2[:k])\n",
    "                    resp_list.append([label, k, remove_stop, 'bigram'] + bigram)\n",
    "                    \n",
    "                    #uni_bi = eval_model(train_journals, test_journals, full_word_list_1_2, remove_stop)\n",
    "                    #stop = eval_model(train_journals, test_journals, full_word_list_stop, remove_stop)\n",
    "                            \n",
    "        column_labels = ['Responsbility/Phase', 'k', 'Removes Stopwords', 'Token Type', 'Train_R', 'Train_F1', 'Test_R', 'Test_F1']\n",
    "        resp_df = pd.DataFrame(data=resp_list, columns=column_labels)\n",
    "        resp_df = resp_df.groupby(['Responsbility/Phase', 'k', 'Removes Stopwords', 'Token Type'])['Train_R', 'Train_F1', 'Test_R', 'Test_F1'].mean()\n",
    "        print(resp_df)\n",
    "        dfs.append(resp_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_resp = annotated_df_resp\n",
    "journals_resp['uni_stop'] = journals_resp['journal_text'].apply(get_grams, args=([1], False))\n",
    "journals_resp['bi_stop'] = journals_resp['journal_text'].apply(get_grams, args=([2], False))\n",
    "journals_resp['uni_nostop'] = journals_resp['journal_text'].apply(get_grams, args=([1], True))\n",
    "journals_resp['bi_nostop'] = journals_resp['journal_text'].apply(get_grams, args=([2], True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srivbane/shared/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/srivbane/shared/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/srivbane/shared/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/srivbane/shared/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "journals_phase = annotated_df_phase\n",
    "journals_phase['uni_stop'] = journals_phase['journal_text'].apply(get_grams, args=([1], False))\n",
    "journals_phase['bi_stop'] = journals_phase['journal_text'].apply(get_grams, args=([2], False))\n",
    "journals_phase['uni_nostop'] = journals_phase['journal_text'].apply(get_grams, args=([1], True))\n",
    "journals_phase['bi_nostop'] = journals_phase['journal_text'].apply(get_grams, args=([2], True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "coordinating_support: 10it [01:19,  8.02s/it]\n",
      "sharing_medical_info: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Train_R  Train_F1  \\\n",
      "Responsbility/Phase  k   Removes Stopwords Token Type                       \n",
      "coordinating_support 10  False             bigram      0.173681  0.295747   \n",
      "                                           unigram     0.165573  0.284091   \n",
      "                         True              bigram      0.187369  0.315288   \n",
      "                                           unigram     0.165573  0.284091   \n",
      "                     100 False             bigram      0.825442  0.904279   \n",
      "                                           unigram     0.673369  0.804704   \n",
      "                         True              bigram      0.867741  0.929110   \n",
      "                                           unigram     0.673369  0.804704   \n",
      "\n",
      "                                                         Test_R   Test_F1  \n",
      "Responsbility/Phase  k   Removes Stopwords Token Type                      \n",
      "coordinating_support 10  False             bigram      0.010749  0.022213  \n",
      "                                           unigram     0.022904  0.055109  \n",
      "                         True              bigram      0.056631  0.096817  \n",
      "                                           unigram     0.022904  0.055109  \n",
      "                     100 False             bigram      0.079606  0.106098  \n",
      "                                           unigram     0.185037  0.190812  \n",
      "                         True              bigram      0.141116  0.166409  \n",
      "                                           unigram     0.185037  0.190324  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sharing_medical_info: 10it [01:55, 11.42s/it]\n",
      "compliance: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Train_R  Train_F1  \\\n",
      "Responsbility/Phase  k   Removes Stopwords Token Type                       \n",
      "sharing_medical_info 10  False             bigram      0.292084  0.451677   \n",
      "                                           unigram     0.280223  0.437609   \n",
      "                         True              bigram      0.335405  0.502196   \n",
      "                                           unigram     0.280223  0.437609   \n",
      "                     100 False             bigram      0.858356  0.923713   \n",
      "                                           unigram     0.796148  0.886419   \n",
      "                         True              bigram      0.899340  0.946981   \n",
      "                                           unigram     0.796491  0.886633   \n",
      "\n",
      "                                                         Test_R   Test_F1  \n",
      "Responsbility/Phase  k   Removes Stopwords Token Type                      \n",
      "sharing_medical_info 10  False             bigram      0.240706  0.379670  \n",
      "                                           unigram     0.274581  0.420127  \n",
      "                         True              bigram      0.308927  0.462655  \n",
      "                                           unigram     0.274581  0.420127  \n",
      "                     100 False             bigram      0.687802  0.787161  \n",
      "                                           unigram     0.642089  0.748799  \n",
      "                         True              bigram      0.738710  0.815333  \n",
      "                                           unigram     0.635452  0.744879  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compliance: 10it [02:14, 13.39s/it]\n",
      "financial_management: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       Train_R  Train_F1  \\\n",
      "Responsbility/Phase k   Removes Stopwords Token Type                       \n",
      "compliance          10  False             bigram      0.202850  0.337189   \n",
      "                                          unigram     0.192952  0.323371   \n",
      "                        True              bigram      0.223385  0.364959   \n",
      "                                          unigram     0.192952  0.323371   \n",
      "                    100 False             bigram      0.742930  0.852456   \n",
      "                                          unigram     0.681811  0.810764   \n",
      "                        True              bigram      0.790076  0.882684   \n",
      "                                          unigram     0.682417  0.811188   \n",
      "\n",
      "                                                        Test_R   Test_F1  \n",
      "Responsbility/Phase k   Removes Stopwords Token Type                      \n",
      "compliance          10  False             bigram      0.165910  0.276377  \n",
      "                                          unigram     0.122096  0.212966  \n",
      "                        True              bigram      0.186797  0.305467  \n",
      "                                          unigram     0.122096  0.212966  \n",
      "                    100 False             bigram      0.550512  0.667428  \n",
      "                                          unigram     0.496980  0.619298  \n",
      "                        True              bigram      0.582513  0.686343  \n",
      "                                          unigram     0.497097  0.617683  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "financial_management: 10it [00:40,  4.30s/it]\n",
      "giving_back: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        Train_R  Train_F1  \\\n",
      "Responsbility/Phase  k   Removes Stopwords Token Type                       \n",
      "financial_management 10  False             bigram      0.436240  0.606859   \n",
      "                                           unigram     0.360217  0.529388   \n",
      "                         True              bigram      0.464501  0.633996   \n",
      "                                           unigram     0.360217  0.529388   \n",
      "                     100 False             bigram      0.948002  0.973272   \n",
      "                                           unigram     0.879245  0.935701   \n",
      "                         True              bigram      0.948002  0.973272   \n",
      "                                           unigram     0.879245  0.935701   \n",
      "\n",
      "                                                         Test_R   Test_F1  \n",
      "Responsbility/Phase  k   Removes Stopwords Token Type                      \n",
      "financial_management 10  False             bigram      0.083333  0.100000  \n",
      "                                           unigram     0.052832  0.064141  \n",
      "                         True              bigram      0.086111  0.103001  \n",
      "                                           unigram     0.052832  0.064141  \n",
      "                     100 False             bigram      0.083333  0.085714  \n",
      "                                           unigram     0.139978  0.085569  \n",
      "                         True              bigram      0.141667  0.118323  \n",
      "                                           unigram     0.139978  0.085569  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "giving_back: 10it [00:52,  5.19s/it]\n",
      "behavior_changes: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       Train_R  Train_F1  \\\n",
      "Responsbility/Phase k   Removes Stopwords Token Type                       \n",
      "giving_back         10  False             bigram      0.352119  0.520312   \n",
      "                                          unigram     0.313301  0.476790   \n",
      "                        True              bigram      0.387340  0.557900   \n",
      "                                          unigram     0.313301  0.476790   \n",
      "                    100 False             bigram      0.987895  0.993906   \n",
      "                                          unigram     0.941752  0.969985   \n",
      "                        True              bigram      0.987895  0.993906   \n",
      "                                          unigram     0.941752  0.969985   \n",
      "\n",
      "                                                        Test_R   Test_F1  \n",
      "Responsbility/Phase k   Removes Stopwords Token Type                      \n",
      "giving_back         10  False             bigram      0.000000  0.000000  \n",
      "                                          unigram     0.000000  0.000000  \n",
      "                        True              bigram      0.016667  0.041667  \n",
      "                                          unigram     0.000000  0.000000  \n",
      "                    100 False             bigram      0.016667  0.025000  \n",
      "                                          unigram     0.109028  0.084618  \n",
      "                        True              bigram      0.066667  0.111302  \n",
      "                                          unigram     0.109028  0.084618  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "behavior_changes: 10it [00:48,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       Train_R  Train_F1  \\\n",
      "Responsbility/Phase k   Removes Stopwords Token Type                       \n",
      "behavior_changes    10  False             bigram      0.299381  0.460483   \n",
      "                                          unigram     0.250043  0.399925   \n",
      "                        True              bigram      0.302704  0.464278   \n",
      "                                          unigram     0.250043  0.399925   \n",
      "                    100 False             bigram      0.990777  0.995356   \n",
      "                                          unigram     0.914726  0.955396   \n",
      "                        True              bigram      0.987454  0.993682   \n",
      "                                          unigram     0.914726  0.955396   \n",
      "\n",
      "                                                        Test_R   Test_F1  \n",
      "Responsbility/Phase k   Removes Stopwords Token Type                      \n",
      "behavior_changes    10  False             bigram      0.000000  0.000000  \n",
      "                                          unigram     0.011111  0.015873  \n",
      "                        True              bigram      0.028219  0.047727  \n",
      "                                          unigram     0.011111  0.015873  \n",
      "                    100 False             bigram      0.012346  0.020202  \n",
      "                                          unigram     0.104437  0.084834  \n",
      "                        True              bigram      0.028219  0.040741  \n",
      "                                          unigram     0.104437  0.084834  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_resp_results = cross_validation(journals_resp, resp_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pretreatment: 10it [07:57, 59.03s/it]\n",
      "treatment: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       Train_R  Train_F1  \\\n",
      "Responsbility/Phase k   Removes Stopwords Token Type                       \n",
      "pretreatment        10  False             bigram      0.080852  0.149571   \n",
      "                                          unigram     0.060295  0.113693   \n",
      "                        True              bigram      0.079838  0.147838   \n",
      "                                          unigram     0.060295  0.113693   \n",
      "                    100 False             bigram      0.452638  0.622798   \n",
      "                                          unigram     0.293782  0.453857   \n",
      "                        True              bigram      0.447140  0.617633   \n",
      "                                          unigram     0.293782  0.453857   \n",
      "\n",
      "                                                        Test_R   Test_F1  \n",
      "Responsbility/Phase k   Removes Stopwords Token Type                      \n",
      "pretreatment        10  False             bigram      0.008547  0.018688  \n",
      "                                          unigram     0.008177  0.026222  \n",
      "                        True              bigram      0.007835  0.015258  \n",
      "                                          unigram     0.008177  0.026222  \n",
      "                    100 False             bigram      0.038832  0.064474  \n",
      "                                          unigram     0.051728  0.060176  \n",
      "                        True              bigram      0.028834  0.042520  \n",
      "                                          unigram     0.051728  0.060188  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "treatment: 10it [25:19, 155.82s/it]\n",
      "end_of_life: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       Train_R  Train_F1  \\\n",
      "Responsbility/Phase k   Removes Stopwords Token Type                       \n",
      "treatment           10  False             bigram      0.130980  0.231312   \n",
      "                                          unigram     0.139101  0.243912   \n",
      "                        True              bigram      0.130690  0.230893   \n",
      "                                          unigram     0.139101  0.243912   \n",
      "                    100 False             bigram      0.476787  0.645357   \n",
      "                                          unigram     0.433610  0.604511   \n",
      "                        True              bigram      0.493213  0.660262   \n",
      "                                          unigram     0.433652  0.604553   \n",
      "\n",
      "                                                        Test_R   Test_F1  \n",
      "Responsbility/Phase k   Removes Stopwords Token Type                      \n",
      "treatment           10  False             bigram      0.034962  0.065620  \n",
      "                                          unigram     0.074543  0.131443  \n",
      "                        True              bigram      0.049651  0.093229  \n",
      "                                          unigram     0.074543  0.131443  \n",
      "                    100 False             bigram      0.292412  0.438208  \n",
      "                                          unigram     0.275333  0.415242  \n",
      "                        True              bigram      0.310188  0.460319  \n",
      "                                          unigram     0.275216  0.415052  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "end_of_life: 10it [09:24, 58.86s/it]\n",
      "cured: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       Train_R  Train_F1  \\\n",
      "Responsbility/Phase k   Removes Stopwords Token Type                       \n",
      "end_of_life         10  False             bigram      0.343836  0.511440   \n",
      "                                          unigram     0.252632  0.403053   \n",
      "                        True              bigram      0.385944  0.556676   \n",
      "                                          unigram     0.252632  0.403053   \n",
      "                    100 False             bigram      0.992296  0.996131   \n",
      "                                          unigram     0.817140  0.899247   \n",
      "                        True              bigram      0.992296  0.996131   \n",
      "                                          unigram     0.817140  0.899247   \n",
      "\n",
      "                                                        Test_R   Test_F1  \n",
      "Responsbility/Phase k   Removes Stopwords Token Type                      \n",
      "end_of_life         10  False             bigram      0.198704  0.302716  \n",
      "                                          unigram     0.091085  0.147279  \n",
      "                        True              bigram      0.210608  0.313803  \n",
      "                                          unigram     0.091085  0.147279  \n",
      "                    100 False             bigram      0.212593  0.260128  \n",
      "                                          unigram     0.098029  0.076279  \n",
      "                        True              bigram      0.259034  0.314662  \n",
      "                                          unigram     0.098029  0.076279  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cured: 10it [12:22, 76.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       Train_R  Train_F1  \\\n",
      "Responsbility/Phase k   Removes Stopwords Token Type                       \n",
      "cured               10  False             bigram      0.108434  0.195578   \n",
      "                                          unigram     0.090335  0.165557   \n",
      "                        True              bigram      0.108425  0.195553   \n",
      "                                          unigram     0.090335  0.165557   \n",
      "                    100 False             bigram      0.532885  0.695005   \n",
      "                                          unigram     0.340112  0.507355   \n",
      "                        True              bigram      0.522911  0.686425   \n",
      "                                          unigram     0.340345  0.507604   \n",
      "\n",
      "                                                        Test_R   Test_F1  \n",
      "Responsbility/Phase k   Removes Stopwords Token Type                      \n",
      "cured               10  False             bigram      0.000000  0.000000  \n",
      "                                          unigram     0.001355  0.003175  \n",
      "                        True              bigram      0.001821  0.005051  \n",
      "                                          unigram     0.001355  0.003175  \n",
      "                    100 False             bigram      0.021852  0.036976  \n",
      "                                          unigram     0.032498  0.035461  \n",
      "                        True              bigram      0.033066  0.044974  \n",
      "                                          unigram     0.032498  0.034919  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_phase_results = cross_validation(journals_phase, phase_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_float(val):\n",
    "    if val >= 0 and val < 0.995:\n",
    "        return \"{:.2f}\".format(val)[1:]\n",
    "    elif val >= 0.995:\n",
    "        return \"1\"\n",
    "    else:\n",
    "        raise ValueError(\"Negatives not handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS & .19 & .32 & .06 & .10 & .87 & .93 & .14 & .17 \\\\\n",
      "SM & .34 & .50 & .31 & .46 & .90 & .95 & .74 & .82 \\\\\n",
      "CP & .22 & .36 & .19 & .31 & .79 & .88 & .58 & .69 \\\\\n",
      "FM & .46 & .63 & .09 & .10 & .95 & .97 & .14 & .12 \\\\\n",
      "GB & .39 & .56 & .02 & .04 & .99 & .99 & .07 & .11 \\\\\n",
      "BC & .30 & .46 & .03 & .05 & .99 & .99 & .03 & .04 \\\\\n"
     ]
    }
   ],
   "source": [
    "# we use bigram, stopwords-removed for reporting, since they have the best test accuracies\n",
    "        \n",
    "# we report only high_irr_responsibility_labels\n",
    "for resp_label, result in zip(resp_subset, grouped_resp_results):\n",
    "    resp_code = responsibility_label_to_code_map[resp_label]\n",
    "    k10_train_r = format_float(result.Train_R[2])\n",
    "    k100_train_r = format_float(result.Train_R[6])\n",
    "    k10_train_f1 = format_float(result.Train_F1[2])\n",
    "    k100_train_f1 = format_float(result.Train_F1[6])\n",
    "    k10_test_r = format_float(result.Test_R[2])\n",
    "    k100_test_r = format_float(result.Test_R[6])\n",
    "    k10_test_f1 = format_float(result.Test_F1[2])\n",
    "    k100_test_f1 = format_float(result.Test_F1[6])\n",
    "    print(f\"{resp_code} & {k10_train_r} & {k10_train_f1} & {k10_test_r} & {k10_test_f1} & {k100_train_r} & {k100_train_f1} & {k100_test_r} & {k100_test_f1} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT & .08 & .15 & .01 & .02 & .45 & .62 & .03 & .04 \\\\\n",
      "T & .13 & .23 & .05 & .09 & .49 & .66 & .31 & .46 \\\\\n",
      "EOL & .39 & .56 & .21 & .31 & .99 & 1 & .26 & .31 \\\\\n",
      "NED & .11 & .20 & .00 & .01 & .52 & .69 & .03 & .04 \\\\\n"
     ]
    }
   ],
   "source": [
    "for phase_code, result in zip([\"PT\", \"T\", \"EOL\", \"NED\"], grouped_phase_results):\n",
    "    k10_train_r = format_float(result.Train_R[2])\n",
    "    k100_train_r = format_float(result.Train_R[6])\n",
    "    k10_train_f1 = format_float(result.Train_F1[2])\n",
    "    k100_train_f1 = format_float(result.Train_F1[6])\n",
    "    k10_test_r = format_float(result.Test_R[2])\n",
    "    k100_test_r = format_float(result.Test_R[6])\n",
    "    k10_test_f1 = format_float(result.Test_F1[2])\n",
    "    k100_test_f1 = format_float(result.Test_F1[6])\n",
    "    print(f\"{phase_code} & {k10_train_r} & {k10_train_f1} & {k10_test_r} & {k10_test_f1} & {k100_train_r} & {k100_train_f1} & {k100_test_r} & {k100_test_f1} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_k_cover_with_counts(journals, k, uniquely_positive_words, resp_label, n_values=[1]):\n",
    "    upw_copy = set(uniquely_positive_words)\n",
    "    \n",
    "    word_list = OrderedDict()\n",
    "    #TODO Generate a list of words\n",
    "    # Breaking ties: at many stages in the algorithm, you'll have multiple words that give you the same improvement in terms of number of documents covered\n",
    "    # When this happens, you should choose randomly.  BUT, we'll want to change this in the future....\n",
    "    \n",
    "    journals = journals.apply(set)\n",
    "    \n",
    "    journals_copy = journals.copy(deep=True)\n",
    "    \n",
    "    # Remove all words that are not uniquely positive words\n",
    "    journals = journals.apply(remove_non_upw, args=(upw_copy,))\n",
    "    \n",
    "    for i in range(k):\n",
    "    #for i in tqdm(range(k), desc=resp_label):\n",
    "        start = time.time()\n",
    "        token_counts = Counter([token for journal in journals for token in journal])\n",
    "        if not token_counts:\n",
    "            break\n",
    "        max_count = max(token_counts.values())\n",
    "        max_word_list = [word for word in token_counts if token_counts[word] == max_count]\n",
    "        max_word_dict = {word:global_word_counts[word] for word in max_word_list}\n",
    "        max_word = break_tie(max_word_dict)\n",
    "\n",
    "        word_list[max_word] = get_journal_count(max_word, journals)\n",
    "        journals = journals[journals.apply(lambda x: max_word not in x)]\n",
    "        \n",
    "    assert len(word_list) <= k\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_word_lists(journals, labels, n=100, bi_nostop_only=False):\n",
    "    for label in labels:\n",
    "        for remove_stop in [True, False]:\n",
    "                uni = 'uni_nostop'\n",
    "                bi = 'bi_nostop'\n",
    "                if remove_stop:\n",
    "                    uni = 'uni_stop'\n",
    "                    bi = 'bi_stop'\n",
    "                    \n",
    "                if bi_nostop_only and not remove_stop:\n",
    "                    continue\n",
    "                \n",
    "                upw_dict_1 = get_uniquely_positive_ngrams(journals, uni, [1], [label])\n",
    "                upw_dict_2 = get_uniquely_positive_ngrams(journals, bi, [2], [label])\n",
    "                full_word_list_1 = max_k_cover_with_counts(journals[uni], n, upw_dict_1[label], label)\n",
    "                full_word_list_2 = max_k_cover_with_counts(journals[bi], n, upw_dict_2[label], label)\n",
    "                \n",
    "                print(label + '-' + uni + '\\n' + str(full_word_list_1), end='\\n')\n",
    "                print(label + '-' + bi + '\\n' + str(full_word_list_2), end='\\n')  \n",
    "                print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coordinating_support-uni_stop\n",
      "OrderedDict([('shay', 8), ('register', 6), ('||www.giveforward.com|fundraiser|0ct0|operationhealthyhootersforkaren', 5), ('showering', 4), ('skeptical', 4), ('retrieval', 4), ('susy', 4), ('.we', 3), ('upsetting', 3), ('literature', 3), ('fr_id|0000', 3), ('dictate', 3), ('momentous', 3), ('montreat', 3), ('lawanda', 3), ('bacteria', 2), ('anyhow', 2), ('rolls', 2), ('momma', 2), ('bangs', 2), ('fade', 2), ('desperate', 2), ('painkillers', 2), ('lowered', 2), ('thoracic', 2), ('kettering', 2), ('confined', 2), ('alike', 2), ('yall', 2), ('monkey', 2)])\n",
      "coordinating_support-bi_stop\n",
      "OrderedDict([('all pooh', 8), ('| encouragement', 6), ('to which', 5), ('| ||www.giveforward.com|fundraiser|0ct0|operationhealthyhootersforkaren', 5), ('dates |', 5), ('| fr_id|0000', 4), ('the albatross', 4), ('from breast', 4), ('everyone so', 4), ('everyone today', 4), ('on without', 3), ('expect that', 3), ('have joined', 3), ('egg retrieval', 3), ('help is', 3), ('dan and', 3), ('to sewanee', 3), ('hit it', 3), ('. deep', 3), ('confirm with', 3), ('akron .', 3), ('exercising |', 3), ('chest pains', 3), ('postings .', 3), ('help during', 3), ('experience on', 3), ('was expected', 3), ('because one', 3), ('the fundraiser', 3), ('over on', 3)])\n",
      "\n",
      "sharing_medical_info-uni_stop\n",
      "OrderedDict([('mastectomy', 86), ('dose', 54), ('physical', 46), ('node', 45), ('asleep', 38), ('discuss', 37), ('november', 35), ('reaction', 33), ('crossed', 30), ('muscle', 27), ('pull', 26), ('genetic', 24), ('travel', 23), ('etc.', 23), ('machine', 21), ('drains', 20), ('denver', 19), ('pooh', 19), ('herceptin', 17), ('general', 17), ('kidney', 17), ('expecting', 16), ('caused', 15), ('injection', 15), ('beth', 15), ('med', 14), ('glass', 14), ('hell', 13), ('temperature', 13), ('fortunately', 12)])\n",
      "sharing_medical_info-bi_stop\n",
      "OrderedDict([('be on', 86), ('i received', 67), ('the port', 61), ('two weeks', 55), ('said i', 49), ('00 days', 46), ('not the', 45), ('doing well', 42), ('have 0', 37), ('surgery is', 36), ('chemo on', 30), ('the lymph', 29), (\"'ll see\", 28), ('radiation treatments', 26), ('bad |', 26), ('am also', 24), ('last treatment', 23), ('since the', 23), ('the breast', 22), ('chemotherapy treatment', 21), ('coming back', 20), ('wednesday |', 20), ('keep my', 19), ('the news', 18), ('done and', 17), ('dr .', 16), ('through |', 16), ('effects |', 16), ('to 00', 15), ('be getting', 15)])\n",
      "\n",
      "compliance-uni_stop\n",
      "OrderedDict([('save', 37), ('headache', 33), ('hooked', 28), ('consult', 27), ('involved', 25), ('|e', 24), ('remaining', 23), ('denver', 23), ('stem', 21), ('swelling', 21), ('hell', 21), ('injection', 20), ('successful', 19), ('severe', 18), ('bloodwork', 17), ('basis', 16), ('presence', 16), ('pace', 15), ('determined', 14), ('gene', 14), ('x|ray', 13), ('scheduling', 13), ('trips', 12), ('tim', 12), ('marjorie', 12), ('worries', 11), ('adriamycin', 11), ('woods', 11), ('shrink', 10), ('screen', 10)])\n",
      "compliance-bi_stop\n",
      "OrderedDict([('i take', 44), ('surgeon .', 41), ('it can', 35), ('if this', 32), ('more chemo', 29), ('left breast', 27), ('doctor and', 24), ('pretty well', 24), ('test .', 24), ('compared to', 23), ('set up', 22), ('he also', 22), ('said he', 20), ('| monday', 20), ('this treatment', 19), ('a double', 18), ('i try', 17), ('then they', 17), ('prior to', 17), ('. during', 16), ('my future', 15), ('folks |', 14), ('the plastic', 14), ('can start', 13), ('next couple', 13), ('to pray', 13), ('the er', 13), ('bone pain', 12), ('deal .', 12), ('comes back', 12)])\n",
      "\n",
      "financial_management-uni_stop\n",
      "OrderedDict([('copay', 3), ('unemployment', 3), ('week|', 2), ('satellite', 2), ('doozy', 2), ('wu', 2), ('ronica', 2), ('pour', 1), ('bake', 1), ('cheap', 1), ('seeking', 1), ('speaker', 1), ('cash', 1), ('intestine', 1), ('guitar', 1), ('cords', 1), ('preventive', 1), ('interviewed', 1), ('te', 1), ('victims', 1), ('disabled', 1), ('prays', 1), ('sponsor', 1), ('curves', 1), ('lymphadema', 1), ('automatic', 1), ('shortest', 1), ('graph', 1), ('extends', 1), ('loser', 1)])\n",
      "financial_management-bi_stop\n",
      "OrderedDict([('about $', 5), ('to penn', 3), ('showed low', 2), ('of forgetting', 2), ('the regular', 2), ('| x', 2), ('preached on', 2), ('which apparently', 2), ('naked |', 2), ('indeed cancer', 2), ('for insurance', 2), ('be bless', 2), ('disability insurance', 2), ('newline $', 2), ('| bleh', 2), ('lotta stuff', 1), ('you straigt', 1), ('tooth |', 1), ('truck would', 1), ('genetics of', 1), ('david not', 1), ('carcinoma which', 1), ('m nervous', 1), ('they not', 1), ('be conferenced', 1), ('favor to', 1), ('was exerting', 1), ('got worked', 1), ('had nana', 1), ('unpredictable .', 1)])\n",
      "\n",
      "giving_back-uni_stop\n",
      "OrderedDict([('fr_id|0000', 4), ('hospice', 3), ('charity', 2), ('effectiveness', 2), ('kerry', 2), ('remiss', 2), ('athlete', 2), ('beaver', 2), ('stepdaughter', 2), ('kimpossible', 2), ('boeckman', 2), ('kisses', 1), ('attacks', 1), ('sheila', 1), ('nite', 1), ('shoe', 1), ('cane', 1), ('conquer', 1), ('carla', 1), ('puncture', 1), ('gum', 1), ('bday', 1), ('minister', 1), ('sterile', 1), ('crawling', 1), ('heroes', 1), ('recliners', 1), ('aml', 1), ('foster', 1), ('websites', 1)])\n",
      "giving_back-bi_stop\n",
      "OrderedDict([('||info.avonfoundation.org|site|tr|walk|washingtondc |', 4), ('acs |', 4), ('my people', 3), ('response .', 3), ('00.0 miles', 3), ('for such', 3), ('support is', 3), ('done my', 3), ('have joined', 3), ('s life', 2), ('lady in', 2), ('girls were', 2), ('this illness', 2), ('hard going', 2), ('put away', 2), ('charity |', 2), ('survivors lap', 2), ('county relay', 2), ('frank said', 2), ('be genetic', 2), ('else someday', 2), ('. hey', 2), ('testing brilliantly', 1), ('then nov.', 1), ('... lots', 1), ('adores her', 1), ('sunday february', 1), ('community i', 1), ('detection saves', 1), ('very lovable', 1)])\n",
      "\n",
      "behavior_changes-uni_stop\n",
      "OrderedDict([('players', 2), ('t0', 2), ('interact', 2), ('scarred', 2), ('jog', 2), ('manhattan', 2), ('jr.', 2), ('cruiser', 2), ('re|evaluation', 2), ('karl', 1), ('pelvic', 1), ('root', 1), ('regards', 1), ('nodule', 1), ('lists', 1), ('wisconsin', 1), ('heidi', 1), ('portland', 1), ('fran', 1), ('thereafter', 1), ('brick', 1), ('ethan', 1), ('cautiously', 1), ('accustomed', 1), ('forgiveness', 1), ('crawling', 1), ('photographer', 1), ('herbal', 1), ('deaf', 1), ('banner', 1)])\n",
      "behavior_changes-bi_stop\n",
      "OrderedDict([('yoga classes', 3), ('. beginning', 2), ('komen race', 2), ('faithful |', 2), ('with arthritis', 2), ('run from', 2), ('terrific .', 2), ('not treating', 2), ('workout videos', 2), ('excuse me', 2), ('. allowing', 2), ('few doctor', 2), ('hurdle .', 2), ('something special', 2), ('lap swimming', 2), ('first days', 2), ('driving over', 2), ('usually spent', 2), ('friend grace', 2), ('spinach |', 2), ('exercise bike', 2), ('... next', 2), ('in awe', 2), ('praise be', 2), ('me productive', 1), ('coach soccer', 1), ('tennis players', 1), ('ut |', 1), (\"'s packaged\", 1), ('talked up', 1)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_word_lists(journals_resp, high_irr_responsibility_labels, n=30, bi_nostop_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretreatment-uni_stop\n",
      "OrderedDict([('gibson', 6), ('her0|neu', 4), ('her0neu', 4), ('hugger', 4), ('stereotactic', 3), ('coh', 3), ('butch', 3), ('iib', 3), ('breast|cancer', 3), ('issac', 3), ('cardio|thoracic', 3), ('fmri', 3), ('lymphona', 3), ('amatruda', 3), ('nwmh', 3), ('morganthaler', 3), ('garrett', 2), ('b|cell', 2), ('post|surgical', 2), ('mist', 2), ('kerri', 2), ('embryos', 2), ('rallying', 2), ('pitty', 2), ('campground', 2), ('gyno', 2), ('trumps', 2), ('p.e.t', 2), ('cartilage', 2), ('allogeneic', 2)])\n",
      "pretreatment-bi_stop\n",
      "OrderedDict([('cervical biopsy', 6), ('. v', 6), ('genomic health', 5), ('biopsied .', 5), ('brca 0', 5), ('could cry', 4), ('0nd breast', 4), ('love rach', 4), ('dr. cooper', 4), ('how likely', 4), ('thinking positively', 4), ('receptors and', 4), ('second look', 4), ('levine cancer', 4), ('genetic blood', 4), ('weekly visits', 3), ('emotional day', 3), (\"'ll speak\", 3), ('dr. that', 3), ('the fmri', 3), ('ironic .', 3), ('ducks in', 3), ('butch and', 3), ('muga |', 3), ('critical |', 3), ('learned was', 3), ('sooner we', 3), ('mediport placed', 3), ('corinne |', 3), ('plan set', 3)])\n",
      "\n",
      "treatment-uni_stop\n",
      "OrderedDict([('chores', 256), ('|karl', 151), ('kittens', 145), ('caretaker', 104), ('transplants', 80), ('aloe', 77), ('encounter', 65), ('shades', 64), ('onc', 57), ('carlie', 55), ('l', 54), ('disconnected', 50), ('interpreter', 49), ('christiana', 49), ('yippee', 48), ('zometa', 48), ('socks', 46), ('ily', 42), ('returns', 38), ('lounge', 38), ('flower', 36), ('external', 36), ('consideration', 36), ('peri', 36), ('postponed', 34), ('des', 33), ('gemzar', 33), ('reflections', 32), ('shade', 31), ('incorporate', 31)])\n",
      "treatment-bi_stop\n",
      "OrderedDict([('low energy', 168), ('way it', 129), ('terrific day', 124), ('the kittens', 103), ('psalms 00|0', 98), ('more treatments', 76), ('maintenance therapy', 69), (\"marie 's\", 66), ('my red', 66), ('session .', 64), ('newline |karl', 64), ('no energy', 63), ('sayin .', 58), ('. |karl', 57), ('chores |', 57), ('is down', 53), ('second round', 50), ('a mental', 49), ('... when', 48), ('this am', 47), ('it usually', 47), ('much energy', 45), ('my neuropathy', 44), ('my chair', 43), ('aloe vera', 43), ('bob sandman', 42), ('play .', 41), ('in delaware', 39), ('way done', 39), ('anti|nausea meds', 39)])\n",
      "\n",
      "end_of_life-uni_stop\n",
      "OrderedDict([('lieu', 9), ('gorospe', 4), ('cemetery', 3), ('heartrate', 2), ('sacrificing', 2), ('caribou', 2), ('on|off', 2), ('promethazine', 2), ('shiva', 2), ('rootbeer', 2), ('calliope', 2), ('leptomeningeal', 2), ('squatty', 2), ('||www.journeycare.org|', 2), ('maddie', 1), ('ver', 1), ('sissy', 1), ('cocky', 1), ('powered', 1), ('ça', 1), ('doris', 1), ('..love', 1), ('gaps', 1), ('soaring', 1), ('fished', 1), ('hallucinations', 1), ('adored', 1), ('alleluia', 1), ('virtues', 1), ('immediatly', 1)])\n",
      "end_of_life-bi_stop\n",
      "OrderedDict([('in lieu', 9), ('our beloved', 7), ('great sadness', 6), ('follow at', 5), ('| hospice', 5), ('began her', 4), ('picc |', 3), ('immunotherapy |', 3), ('my hospice', 3), ('smile because', 3), ('service will', 3), ('nursing visit', 2), ('that julie', 2), ('aunt nancy', 2), ('newline visitation', 2), ('jim passed', 2), ('her funeral', 2), ('and hospice', 2), ('her sisters', 2), (\"'s ashes\", 2), ('what hospice', 2), ('. rich', 2), ('biscuit is', 2), ('newline filling', 2), ('ça change', 1), ('available they', 1), ('simply mowed', 1), ('laughter at', 1), ('levy called', 1), ('darlys is', 1)])\n",
      "\n",
      "cured-uni_stop\n",
      "OrderedDict([('rashid', 10), ('ogburn', 8), ('peoria', 5), ('mammaprint', 5), ('hemolysis', 4), ('avanti', 4), ('scan|', 3), ('camp|', 3), ('zoberri', 3), ('repetitive', 2), ('0lbs', 2), ('vase', 2), ('duper', 2), ('revision', 2), ('quinn', 2), ('breakdowns', 2), ('carpal', 2), ('insecurity', 2), ('touchy', 2), ('thymus', 2), ('embarked', 2), ('maplewood', 2), ('us|', 2), ('doves', 2), ('swims', 2), ('pitches', 2), ('day|and', 2), ('sniffed', 2), ('aromas', 2), ('shaklee', 2)])\n",
      "cured-bi_stop\n",
      "OrderedDict([('dr. rashid', 10), ('dr. ogburn', 8), ('sharing our', 6), ('arm exercises', 6), ('stomach doctor', 5), ('dad full', 4), ('| wes', 4), ('day |000', 4), ('newline peggy', 4), ('definitely happy', 4), ('saline fill', 4), ('implants and', 4), ('officially cancer', 4), ('my mammaprint', 4), ('rainbows are', 4), ('was june', 3), ('know over', 3), ('each team', 3), ('vulcans were', 3), ('of choices', 3), ('post diep', 3), ('the saga', 3), ('blogging ...', 3), ('cancer|free for', 3), ('legs waxed', 3), ('tommy will', 3), ('florida keys', 3), ('reno .', 3), ('maintenance phase', 3), ('tried calling', 3)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_word_lists(journals_phase, phase_labels, n=30, bi_nostop_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coordinating_support-uni_stop\n",
      "OrderedDict([('shay', 8), ('register', 6), ('||www.giveforward.com|fundraiser|0ct0|operationhealthyhootersforkaren', 5), ('showering', 4), ('skeptical', 4), ('retrieval', 4), ('susy', 4), ('.we', 3), ('upsetting', 3), ('literature', 3), ('fr_id|0000', 3), ('dictate', 3), ('momentous', 3), ('montreat', 3), ('lawanda', 3), ('bacteria', 2), ('anyhow', 2), ('rolls', 2), ('momma', 2), ('bangs', 2), ('fade', 2), ('desperate', 2), ('painkillers', 2), ('lowered', 2), ('thoracic', 2), ('kettering', 2), ('confined', 2), ('alike', 2), ('yall', 2), ('monkey', 2), ('|the', 2), ('incurable', 2), ('spontaneous', 2), ('detox', 2), ('|and', 2), ('queasiness', 2), ('magnitude', 2), ('joyfully', 2), ('cardinal', 2), ('scent', 2), ('tote', 2), ('onslaught', 2), ('rallying', 2), ('aisles', 2), ('whelming', 2), ('lagging', 2), ('re|evaluation', 2), ('re|connected', 2), ('ovulation', 2), ('kaminsky', 2), ('amatruda', 2), ('joan', 1), ('opposed', 1), ('maggie', 1), ('sheila', 1), ('fl', 1), ('prevention', 1), ('chili', 1), ('perry', 1), ('jimmy', 1), ('dryer', 1), ('noah', 1), ('cardiac', 1), ('guiding', 1), ('autumn', 1), ('harley', 1), ('bday', 1), ('cords', 1), ('jar', 1), ('chamber', 1), ('forecast', 1), ('demand', 1), ('remedies', 1), ('oak', 1), ('aml', 1), ('everyones', 1), ('refrain', 1), ('usa', 1), ('darlene', 1), ('owners', 1), ('choked', 1), ('creator', 1), ('clears', 1), ('contacting', 1), ('anti|cancer', 1), ('owned', 1), ('indefinitely', 1), ('mammography', 1), ('metro', 1), ('edit', 1), ('medal', 1), ('bi|lateral', 1), ('immunotherapy', 1), ('fractures', 1), ('grains', 1), ('.love', 1), ('slammed', 1), ('illusion', 1), ('calendars', 1), ('inspires', 1)])\n",
      "\n",
      "coordinating_support-bi_stop\n",
      "OrderedDict([('all pooh', 8), ('| encouragement', 6), ('| ||www.giveforward.com|fundraiser|0ct0|operationhealthyhootersforkaren', 5), ('to which', 5), ('dates |', 5), ('| ||info.avonfoundation.org|site|tr|walk|washingtondc', 4), ('the albatross', 4), ('from breast', 4), ('. pooh', 4), ('hit it', 3), ('implants and', 3), ('hitting me', 3), ('have joined', 3), ('i strongly', 3), ('dan and', 3), ('gets the', 3), ('postings .', 3), ('exercising |', 3), ('experience on', 3), ('feel loved', 3), ('expect that', 3), ('to montreat', 3), ('on without', 3), ('to donate', 3), ('thre |', 3), ('confirm with', 3), ('help during', 3), ('was expected', 3), ('over on', 3), ('the fundraiser', 3), ('because one', 3), ('akron .', 3), ('everyone so', 3), ('info for', 2), ('site has', 2), ('you. |', 2), ('fine newline', 2), ('table again', 2), ('sloan kettering', 2), ('reading in', 2), ('egg harvesting', 2), ('knowing me', 2), ('town to', 2), ('coming ||', 2), ('ball or', 2), ('a far', 2), ('hear your', 2), ('they monitor', 2), ('ask your', 2), ('my teacher', 2), ('tumor gone', 2), ('dr. rad', 2), ('a must', 2), ('considering going', 2), ('my spouse', 2), ('pump me', 2), ('spend all', 2), ('medical care', 2), ('those gorgeous', 2), ('everyone today', 2), ('and sharing', 2), ('some folks', 2), ('county relay', 2), ('rallying around', 2), ('after friday', 2), ('scalp aches', 2), (\"'s ct\", 2), ('us pray', 2), ('continue through', 2), ('this event', 2), ('at shady', 2), ('salt water', 2), ('tumor now', 2), ('is healthy', 2), ('surgeon again', 2), ('usually get', 2), ('unwind .', 2), ('| fourth', 2), ('sleeping for', 2), ('bump .', 2), ('| lawanda', 2), ('can manage', 2), ('thing because', 2), ('please think', 2), ('is involvement', 1), ('reluctantly went', 1), ('boo boo', 1), ('nodes it', 1), ('little overboard', 1), ('polly ps', 1), ('bash at', 1), ('serious has', 1), ('people find', 1), ('specialists in', 1), ('taxotere affects', 1), ('usa thank', 1), ('marilou sends', 1), ('technology will', 1), ('breaks to', 1), ('small next', 1)])\n",
      "\n",
      "\n",
      "coordinating_support-uni_nostop\n",
      "OrderedDict([('shay', 8), ('register', 6), ('||www.giveforward.com|fundraiser|0ct0|operationhealthyhootersforkaren', 5), ('showering', 4), ('skeptical', 4), ('retrieval', 4), ('susy', 4), ('.we', 3), ('upsetting', 3), ('literature', 3), ('fr_id|0000', 3), ('dictate', 3), ('momentous', 3), ('montreat', 3), ('lawanda', 3), ('bacteria', 2), ('anyhow', 2), ('rolls', 2), ('momma', 2), ('bangs', 2), ('fade', 2), ('desperate', 2), ('painkillers', 2), ('lowered', 2), ('thoracic', 2), ('kettering', 2), ('confined', 2), ('alike', 2), ('yall', 2), ('monkey', 2), ('|the', 2), ('incurable', 2), ('spontaneous', 2), ('detox', 2), ('|and', 2), ('queasiness', 2), ('magnitude', 2), ('joyfully', 2), ('cardinal', 2), ('scent', 2), ('tote', 2), ('onslaught', 2), ('rallying', 2), ('aisles', 2), ('whelming', 2), ('lagging', 2), ('re|evaluation', 2), ('re|connected', 2), ('ovulation', 2), ('kaminsky', 2), ('amatruda', 2), ('joan', 1), ('opposed', 1), ('maggie', 1), ('sheila', 1), ('fl', 1), ('prevention', 1), ('chili', 1), ('perry', 1), ('jimmy', 1), ('dryer', 1), ('noah', 1), ('cardiac', 1), ('guiding', 1), ('autumn', 1), ('harley', 1), ('bday', 1), ('cords', 1), ('jar', 1), ('chamber', 1), ('forecast', 1), ('demand', 1), ('remedies', 1), ('oak', 1), ('aml', 1), ('everyones', 1), ('refrain', 1), ('usa', 1), ('darlene', 1), ('owners', 1), ('choked', 1), ('creator', 1), ('clears', 1), ('contacting', 1), ('anti|cancer', 1), ('owned', 1), ('indefinitely', 1), ('mammography', 1), ('metro', 1), ('edit', 1), ('bi|lateral', 1), ('medal', 1), ('immunotherapy', 1), ('grains', 1), ('fractures', 1), ('.love', 1), ('slammed', 1), ('illusion', 1), ('calendars', 1), ('inspires', 1)])\n",
      "\n",
      "coordinating_support-bi_nostop\n",
      "OrderedDict([('love pooh', 9), ('| ||www.giveforward.com|fundraiser|0ct0|operationhealthyhootersforkaren', 5), ('dates |', 5), ('| ||info.avonfoundation.org|site|tr|walk|washingtondc', 4), ('creative |', 4), ('thre |', 4), ('the nausea', 4), ('. pooh', 4), ('asking help', 3), ('newline sister', 3), ('celebrate 00th', 3), ('thanks wonderful', 3), ('be grateful', 3), ('surgery take', 3), ('|| trigger', 3), ('affects .', 3), ('joining team', 3), ('chest pains', 3), ('system compromised', 3), ('akron .', 3), ('start grow', 3), ('cart |', 3), ('my cousin', 3), ('really working', 3), ('nair said', 3), ('happy st.', 3), ('bras |', 3), ('see mind', 2), ('my sisters', 2), ('sometime around', 2), ('... .we', 2), ('hours right', 2), ('house right', 2), ('linked breast', 2), ('| lawanda', 2), ('yall |', 2), ('rides .', 2), ('mapped .', 2), ('please think', 2), ('already come', 2), ('two separate', 2), ('scalp aches', 2), ('today monday', 2), ('node removal', 2), ('hour surgery', 2), ('skeptical |', 2), ('think gone', 2), ('spots right', 2), ('cooperated |', 2), ('hands feel', 2), ('salt water', 2), ('i cured', 2), ('. ps', 2), (\"'ve exercising\", 2), ('newline friends', 2), ('| cross', 2), ('without surgery', 2), ('risen lord', 2), ('lowest point', 2), ('said breast', 2), ('may think', 2), ('pictures 00th', 2), ('little silly', 2), (\"n't seeing\", 2), ('d levels', 2), ('simple cysts', 2), ('our goal', 2), ('reading please', 2), ('pick 0', 2), ('here website', 2), ('keep calling', 2), ('ask continue', 2), ('worst day', 2), (\"'s need\", 2), ('duty iv', 2), ('cover i', 2), ('00 eggs', 2), ('irony i', 2), ('definitely road', 2), ('possibilities include', 2), ('constant ache', 2), ('flowing way', 2), ('medical care', 2), ('battle breast', 2), ('taking last', 1), ('shrink tumors', 1), ('enjoying seeing', 1), ('pay fertility', 1), ('susan trying', 1), ('sharing journey', 1), ('stop make', 1), ('goes edge', 1), ('million stem', 1), ('count dipped', 1), ('bald back', 1), ('especially want', 1), ('appreciate written', 1), ('helium newline', 1), (\"'s boo\", 1), ('expanders really', 1)])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_word_lists(journals_resp, ['coordinating_support'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treatment-uni_stop\n",
      "OrderedDict([('chores', 256), ('|karl', 151), ('kittens', 145), ('caretaker', 104), ('transplants', 80), ('aloe', 77), ('encounter', 65), ('shades', 64), ('onc', 57), ('carlie', 55), ('interpreter', 50), ('disconnected', 50), ('yippee', 49), ('zometa', 49), ('christiana', 49), ('ily', 49), ('socks', 46), ('l', 39), ('returns', 38), ('lounge', 38), ('flower', 36), ('external', 36), ('consideration', 36), ('peri', 36), ('postponed', 34), ('des', 33), ('gemzar', 33), ('reflections', 32), ('shade', 31), ('incorporate', 31), ('.0', 30), ('i.e', 30), ('feedback', 30), ('repair', 29), ('handful', 28), ('bore', 28), ('skipped', 28), ('clumps', 27), ('intravenous', 27), ('caleb', 27), ('soldier', 27), ('calories', 26), ('trend', 26), ('linings', 26), ('acupuncturist', 26), ('a.m', 25), ('considerably', 25), ('phillip', 25), ('tougher', 24), ('nuisance', 24), ('safa', 24), ('sac', 23), ('inactive', 23), ('oct.', 22), ('accompanying', 22), ('sioux', 22), ('delightful', 21), ('jess', 21), ('olivia', 21), ('lemma', 21), ('implement', 21), ('ginger', 20), ('jaw', 20), ('animals', 20), ('sized', 20), ('shifting', 20), ('maximize', 20), ('cookie', 19), ('theory', 19), ('tarceva', 19), ('margo', 19), ('antigen', 19), ('macbook', 19), ('zofran', 18), ('happily', 18), ('spirituality', 18), ('newark', 18), ('accessed', 17), ('thrush', 17), ('bean', 17), ('aggravated', 17), ('queasy', 16), ('jeans', 16), ('troubles', 16), ('thirty', 16), ('competition', 16), ('strict', 16), ('pumps', 16), ('unchanged', 16), ('jerome', 16), ('jani', 16), ('tap', 15), ('whining', 15), ('mighty', 15), ('immunity', 15), ('incurable', 15), ('tue', 15), ('documents', 15), ('crackers', 14), ('dots', 14)])\n",
      "\n",
      "treatment-bi_stop\n",
      "OrderedDict([('low energy', 168), ('way it', 129), ('terrific day', 124), ('the kittens', 103), ('psalms 00|0', 98), ('more treatments', 76), ('chores |', 69), ('legs and', 68), ('newline |karl', 64), ('session .', 63), ('much energy', 60), ('chemo yesterday', 60), ('my red', 57), ('. |karl', 57), ('sayin .', 57), (\"'s sleep\", 54), ('of living', 54), ('no energy', 51), ('is down', 51), ('second round', 48), ('it usually', 47), ('... when', 47), ('this am', 46), ('chemo pills', 45), (\"chemo ''\", 44), ('aloe vera', 43), ('my neuropathy', 42), ('my challenge', 42), ('bob sandman', 41), ('way done', 40), ('go again', 38), ('to mind', 36), ('white cells', 36), (\"n't updated\", 36), ('round .', 35), ('love darlys', 34), ('mid 00', 34), ('an appetite', 33), ('felt really', 33), ('to ride', 33), ('my onc', 33), ('as in', 32), ('center for', 32), ('and stamina', 32), ('i assume', 31), ('love deb', 30), ('a.m .', 30), ('anti|nausea meds', 30), ('tea |', 29), ('hearing from', 28), ('on is', 28), ('oral chemo', 28), ('probably will', 28), ('my platelet', 27), ('through each', 27), ('des moines', 27), ('and letting', 27), ('big chemo', 26), ('0 down', 26), ('to wisconsin', 26), ('draw |', 26), ('memorial sloan', 26), ('the birds', 25), ('a mental', 25), ('chemo appointment', 25), ('and realized', 25), ('doing its', 25), ('felt pretty', 25), ('slowed down', 24), ('love ines', 24), ('transplant team', 24), ('kinda like', 24), ('the interpreter', 23), ('yoga class', 23), ('simple .', 23), ('fluids and', 23), ('br newline', 22), ('dr on', 22), ('take so', 22), ('yippee |', 22), ('cancer markers', 22), ('on maintenance', 22), ('a thought', 21), ('| becky', 21), ('. jess', 21), ('dr. lemma', 21), ('up tomorrow', 21), ('like crap', 20), ('of thanks', 20), ('chemo last', 20), ('sessions |', 20), (\"'ve experienced\", 20), (\"'m eating\", 20), ('good start', 19), ('pulled out', 19), ('newline becky', 19), ('treatment went', 19), ('my skull', 19), ('treatments have', 19), ('on wood', 18)])\n",
      "\n",
      "\n",
      "treatment-uni_nostop\n",
      "OrderedDict([('chores', 256), ('|karl', 151), ('kittens', 145), ('caretaker', 104), ('transplants', 80), ('aloe', 77), ('encounter', 65), ('shades', 64), ('onc', 57), ('carlie', 55), ('interpreter', 50), ('disconnected', 50), ('yippee', 49), ('zometa', 49), ('christiana', 49), ('ily', 49), ('socks', 46), ('l', 39), ('returns', 38), ('lounge', 38), ('flower', 36), ('external', 36), ('consideration', 36), ('peri', 36), ('postponed', 34), ('des', 33), ('gemzar', 33), ('reflections', 32), ('shade', 31), ('incorporate', 31), ('.0', 30), ('i.e', 30), ('feedback', 30), ('repair', 29), ('handful', 28), ('bore', 28), ('skipped', 28), ('clumps', 27), ('intravenous', 27), ('caleb', 27), ('soldier', 27), ('calories', 26), ('trend', 26), ('acupuncturist', 26), ('linings', 26), ('a.m', 25), ('considerably', 25), ('phillip', 25), ('tougher', 24), ('nuisance', 24), ('safa', 24), ('sac', 23), ('inactive', 23), ('oct.', 22), ('accompanying', 22), ('sioux', 22), ('delightful', 21), ('jess', 21), ('olivia', 21), ('lemma', 21), ('implement', 21), ('ginger', 20), ('jaw', 20), ('animals', 20), ('sized', 20), ('shifting', 20), ('maximize', 20), ('cookie', 19), ('theory', 19), ('tarceva', 19), ('margo', 19), ('antigen', 19), ('macbook', 19), ('zofran', 18), ('happily', 18), ('spirituality', 18), ('newark', 18), ('accessed', 17), ('thrush', 17), ('bean', 17), ('aggravated', 17), ('queasy', 16), ('jeans', 16), ('troubles', 16), ('thirty', 16), ('competition', 16), ('strict', 16), ('pumps', 16), ('unchanged', 16), ('jerome', 16), ('jani', 16), ('tap', 15), ('whining', 15), ('mighty', 15), ('immunity', 15), ('incurable', 15), ('tue', 15), ('documents', 15), ('crackers', 14), ('dots', 14)])\n",
      "\n",
      "treatment-bi_nostop\n",
      "OrderedDict([('marie i', 225), ('low energy', 161), ('psalms 00|0', 98), ('terrific day', 94), ('have beautiful', 91), ('maintenance therapy', 70), ('newline |karl', 64), ('. |karl', 60), ('sayin .', 59), ('still hair', 58), ('i rest', 58), ('session .', 57), ('marie |', 57), (\"| 'm\", 56), (\"'s sleep\", 54), ('chemo yesterday', 49), ('round .', 48), ('chemo pills', 47), ('today another', 47), ('br newline', 45), ('bob sandman', 44), ('second round', 43), ('way done', 42), ('white cells', 40), ('tumor marker', 39), ('i scan', 38), ('lesson |', 37), ('chores |', 37), ('00 go', 37), ('a.m .', 37), (\"marie 's\", 36), (\"n't updated\", 36), ('| frustrating', 35), ('love deb', 34), ('thanks reading', 34), ('anti|nausea meds', 33), ('big chemo', 33), ('kinda like', 33), ('platelet count', 32), ('upper 00', 32), ('oral chemo', 31), ('maintenance |', 31), ('des moines', 30), ('more |', 29), ('woke feeling', 29), ('want feel', 29), ('simple .', 29), ('treatments go', 29), (\"'ve mentioned\", 29), ('treatment went', 28), ('felt pretty', 27), ('chemo appointment', 27), ('felt much', 27), ('aloe vera', 27), ('two treatments', 27), ('one drugs', 27), ('eating well', 26), ('i assume', 26), ('cancer markers', 26), ('felt really', 26), ('memorial sloan', 26), ('delaware .', 25), ('... when', 25), ('i pee', 25), ('level low', 25), ('de pink', 24), ('ouch .', 24), ('| |karl', 24), ('yippee |', 24), ('love ines', 24), ('love darlys', 23), ('i half', 23), ('part process', 23), ('counts low', 23), ('get two', 22), ('little pony', 22), ('draw |', 22), ('dr. lemma', 21), ('| becky', 21), ('hi folks', 21), ('like crap', 21), ('pleased progress', 21), ('motivation .', 21), ('him |', 20), ('no wonder', 20), ('bones |', 20), ('he thinks', 20), (\"who 's\", 20), ('. slept', 19), ('sessions |', 19), ('emails .', 19), ('newline becky', 19), ('treatment next', 19), ('chemo tuesday', 18), ('shot i', 18), ('much work', 18), ('fatigued .', 18), ('the challenge', 18), ('| carlie', 18), ('total .', 18)])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_word_lists(journals_phase, ['treatment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(1, 1), (2, 2)])\n"
     ]
    }
   ],
   "source": [
    "a = OrderedDict()\n",
    "a[1] = 1\n",
    "a[2] = 2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Zach's Pseudocode\n",
    "\n",
    "# journals = get annotated journals()\n",
    "for resp_label in high_irr_responsibility_labels (6 total):  # note that this list actually does exist if you import from the responsibility package...\n",
    "    train_recalls = []\n",
    "    train_f2s = []\n",
    "    test_recalls = []\n",
    "    test_f2s = []\n",
    "    # there is a scikit-learn method that creates CrossValidation splits and lets you iterate through them\n",
    "    cv_splits = get 10-fold cross validation splits of journals\n",
    "    for train_journals, test_journals in cv_splits:  # there will be 10 iterations of this loop, since we're using 10-fold cross validation\n",
    "        train_pos_journals = train_journals that are annotated with resp_label  # in other words, resp_label_score >= 0.5...\n",
    "        full_wordlist = get_word_list(train_pos_journals, k=100)\n",
    "        train_true = labels of the train_pos_journals\n",
    "        test_true = labels of the test_pos_journals\n",
    "        for k in [10, 100]:\n",
    "            wordlist = full_wordlist[:k]\n",
    "            # word_in_journal returns 1 if the journal matches any of the tokens in the wordlist, 0 otherwise\n",
    "            train_predicted = [word_in_journal(wordlist, journal) for journal in train_journals]\n",
    "            train_recall = sklearn.metrics.recall_score(train_true, train_predicted)\n",
    "            train_f2 = sklearn.metrics.fbeta_score(train_true, train_predicted, 2)\n",
    "            test_predicted = [word_in_journal(wordlist, journal) for journal in test_journals]\n",
    "            test_recall = sklearn.metrics.recall_score(test_true, test_predicted)\n",
    "            test_f2 = sklearn.metrics.fbeta_score(test_true, test_predicted, 2)\n",
    "            train_recalls.append(train_recall)\n",
    "            train_f2s.append(train_f2)\n",
    "            test_recalls.append(test_recall)\n",
    "            test_f2s.append(test_f2)\n",
    "    mean_train_recall = mean(train_recalls)\n",
    "    mean_train_f2 = mean(train_f2s)\n",
    "    mean_test_recall = mean(test_recalls)\n",
    "    mean_test_f2 = mean(test_f2s)\n",
    "    save(mean_train_recall, mean_train_f2, mean_test_recall, mean_test_f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keyword_list = [sharing, medical, information]\n",
    "actual_perfect_list = [ 100 words... ]\n",
    "pretty_good_list, guaranteed to be within some % of the actual_perfect_list = [ ... ]\n",
    "our_model does much better than both of these!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% Precision\n",
    "    The words on the keyword list must appear only in the \"positive\" documents for a particular responsibility\n",
    "    It can't be contained in the negative documents\n",
    "What % recall can we get given that we can select *k* words?\n",
    "\n",
    "k = 2\n",
    "pretty_good_list = [word1, word2]\n",
    "recall = 40%\n",
    "\n",
    "k = 3\n",
    "pretty_good_list = [word1, word2, word3]\n",
    "recall = 42%\n",
    "\n",
    "k = 10, 50, 100, 500, 1000\n",
    "what is the recall at each of these values of k?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this a maximum coverage problem?\n",
    "Or rather a \"Max k-Cover\" problem\n",
    "\n",
    "Choose pretty_good_list ONLY from words that appear only in the positive documents\n",
    "len(pretty_good_list) == k\n",
    "AND recall is maximized\n",
    "\n",
    "Uniquely Positive Words = [word1, word2, word3]\n",
    "\n",
    "k=1\n",
    "word1 appears in 5 documents\n",
    "word2 appears in 20\n",
    "word3 appears in 15\n",
    "\n",
    "Set word1 = [d1, d2, d3, d4, d5]\n",
    "Set word2 = [... but NOT d3 and d5]\n",
    "Set word3 = [ ...... ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
